<p>In <a href="https://aws.amazon.com/s3/" target="_blank" rel="noopener">Amazon Simple Storage Service (Amazon S3)</a>, you can use <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html" target="_blank" rel="noopener">cross-region replication (CRR)</a> to copy objects automatically and asynchronously across buckets in different AWS Regions. CRR is a bucket-level configuration, and it can help you meet compliance requirements and minimize latency by keeping copies of your data in different Regions. CRR replicates all objects in the source bucket, or optionally a subset, controlled by prefix and tags.</p> 
<p>Objects that exist before you enable CRR (<em>pre-existing</em> objects) are not replicated. Similarly, objects might fail to replicate (<em>failed</em> objects) if <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/setting-repl-config-perm-overview.html" target="_blank" rel="noopener">permissions</a> aren’t in place, either on the IAM role used for replication or the bucket policy (if the buckets are in different AWS accounts).</p> 
<p>In our work with customers, we have seen situations where large numbers of objects aren’t replicated for the previously mentioned reasons. In this post, we show you how to trigger cross-region replication for pre-existing and failed objects.</p> 
<h2>Methodology</h2> 
<p>At a high level, our strategy is to perform a copy-in-place operation on pre-existing and failed objects. This operation uses the Amazon S3 API to copy the objects over the top of themselves, preserving tags, access control lists (ACLs), metadata, and encryption keys. The operation also resets the <code>Replication_Status</code> flag on the objects. This triggers cross-region replication, which then copies the objects to the destination bucket.</p> 
<p>To accomplish this, we use the following:</p> 
<ul> 
 <li><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html" target="_blank" rel="noopener">Amazon S3 inventory</a> to identify objects to copy in place. These objects don’t have a replication status, or they have a status of FAILED.</li> 
 <li><a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener">Amazon Athena</a> and <a href="https://aws.amazon.com/glue/" target="_blank" rel="noopener">AWS Glue</a> to expose the S3 inventory files as a table.</li> 
 <li><a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener">Amazon EMR</a> to execute an Apache Spark job that queries the AWS Glue table and performs the copy-in-place operation.</li> 
</ul> 
<h3>Object filtering</h3> 
<p>To reduce the size of the problem (we’ve seen buckets with billions of objects!) and eliminate S3 List operations, we use Amazon S3 inventory. S3 inventory is enabled at the bucket level, and it provides a report of S3 objects. The inventory files contain the objects’ replication status: PENDING, COMPLETED, FAILED, or REPLICA. Pre-existing objects do&nbsp;<em>not</em>&nbsp;have a replication status in the inventory.</p> 
<h3>Interactive analysis</h3> 
<p>To simplify working with the files that are created by S3 inventory, we create a table in the AWS Glue Data Catalog. You can query this table using Amazon Athena and analyze the objects. &nbsp;You can also use this table in the Spark job running on Amazon EMR to identify the objects to copy in place.</p> 
<h3>Copy-in-place execution</h3> 
<p>We use a Spark job running on Amazon EMR to perform concurrent copy-in-place operations of the S3 objects. This step allows the number of simultaneous copy operations to be scaled up. This improves performance on a large number of objects compared to doing the copy operations consecutively with a single-threaded application.</p> 
<h2>Account setup</h2> 
<p>For the purpose of this example, we created three S3 buckets. The buckets are specific to our demonstration. If you’re following along, you need to create your own buckets (with different names).</p> 
<p>We’re using a source bucket named <code>crr-preexisting-demo-source</code> and a destination bucket named <code>crr-preexisting-demo-destination</code>. The source bucket contains the pre-existing objects and the objects with the replication status of FAILED. We store the S3 inventory files in a third bucket named&nbsp;<code>crr-preexisting-demo-inventory</code>.</p> 
<p>The following diagram illustrates the basic setup.</p> 
<p><img class="alignnone size-full wp-image-6958" style="margin: 20px 0px 20px 0px" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/TriggerCrossS3EMR1.png" alt="" width="636" height="375" /></p> 
<p>You can use any bucket to store the inventory, but the bucket policy must include the following statement (change <code>Resource</code> and <code>aws:SourceAccount</code> to match yours).</p> 
<div class="hide-language"> 
 <pre><code class="lang-code">{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Id&quot;: &quot;S3InventoryPolicy&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;S3InventoryStatement&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Principal&quot;: {
                &quot;Service&quot;: &quot;s3.amazonaws.com&quot;
            },
            &quot;Action&quot;: &quot;s3:PutObject&quot;,
            &quot;Resource&quot;: &quot;arn:aws:s3:::crr-preexisting-demo-inventory/*&quot;,
            &quot;Condition&quot;: {
                &quot;StringEquals&quot;: {
                    &quot;s3:x-amz-acl&quot;: &quot;bucket-owner-full-control&quot;,
                    &quot;aws:SourceAccount&quot;: &quot;111111111111&quot;
                }
            }
        }
    ]
}
</code></pre> 
</div> 
<p>In our example, we uploaded six objects to&nbsp;<code>crr-preexisting-demo-source</code>. We added three objects (<code>preexisting-*.txt</code>) before CRR was enabled. We also added three objects (<code>failed-*.txt</code>) after permissions were removed from the CRR IAM role, causing CRR to fail.</p> 
<p><img class="alignnone size-full wp-image-6959" style="margin: 20px 0px 20px 0px;border: 1px solid #cccccc" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/TriggerCrossS3EMR2.png" alt="" width="300" height="432" /></p> 
<h2>Enable S3 inventory</h2> 
<p>You need to enable S3 inventory on the source bucket. You can do this on the Amazon S3 console as follows:</p> 
<p>On the <strong>Management</strong> tab for the source bucket, choose <strong>Inventory</strong>.</p> 
<p><img class="alignnone size-full wp-image-6960" style="margin: 20px 0px 20px 0px;border: 1px solid #cccccc" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/TriggerCrossS3EMR3.png" alt="" width="800" height="325" /></p> 
<p>Choose <strong>Add new</strong>, and complete the settings as shown, choosing the <strong>CSV</strong> format and selecting the <strong>Replication status</strong> check box. For detailed instructions for creating an inventory, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-inventory.html" target="_blank" rel="noopener">How Do I Configure Amazon S3 Inventory?</a> in the <em>Amazon S3 Console User Guide</em>.</p> 
<p><img class="alignnone size-full wp-image-6961" style="margin: 20px 0px 20px 0px;border: 1px solid #cccccc" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/TriggerCrossS3EMR4.png" alt="" width="800" height="397" /></p> 
<p>After enabling S3 inventory, you need to wait for the inventory files to be delivered. It can take up to 48 hours to deliver the first report. If you’re following the demo, ensure that the inventory report is delivered before proceeding.</p> 
<p>Here’s what our example inventory file looks like:</p> 
<p><img class="alignnone size-full wp-image-6962" style="margin: 20px 0px 20px 0px;border: 1px solid #cccccc" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/TriggerCrossS3EMR5.png" alt="" width="800" height="183" /></p> 
<p>You can also look on the S3 console on the objects’ <strong>Overview</strong> tab. The pre-existing objects do not have a replication status, but the failed objects show the following:</p> 
<p><img class="alignnone size-full wp-image-6963" style="margin: 20px 0px 20px 0px;border: 1px solid #cccccc" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/TriggerCrossS3EMR6.png" alt="" width="342" height="108" /></p> 
<h2>Register the table in the AWS Glue Data Catalog using Amazon Athena</h2> 
<p>To be able to query the inventory files using SQL, first you need to create an external table in the AWS Glue Data Catalog. Open the Amazon Athena console at <a href="https://console.aws.amazon.com/athena/home" target="_blank" rel="noopener">https://console.aws.amazon.com/athena/home</a>.</p> 
<p>On the <strong>Query Editor</strong> tab, run the following SQL statement. This statement registers the external table in the AWS Glue Data Catalog.</p> 
<div class="hide-language"> 
 <pre><code class="lang-sql">CREATE EXTERNAL TABLE IF NOT EXISTS
crr_preexisting_demo (
    `bucket` string,
    key string,
    replication_status string
)
PARTITIONED BY (dt string)
ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ','
    ESCAPED BY '\\'
    LINES TERMINATED BY '\n'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
LOCATION 's3://crr-preexisting-demo-inventory/crr-preexisting-demo-source/crr-preexisting-demo/hive';</code></pre> 
</div> 
<p>After creating the table, you need to make the AWS Glue Data Catalog aware of any existing data and partitions by adding partition metadata to the table. To do this, you use the Metastore Consistency Check utility to scan for and add partition metadata to the AWS Glue Data Catalog.</p> 
<div class="hide-language"> 
 <pre><code class="lang-sql">MSCK REPAIR TABLE crr_preexisting_demo;</code></pre> 
</div> 
<p>To learn more about why this is required, see the documentation on <a href="https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html" target="_blank" rel="noopener">MSCK REPAIR TABLE</a> and <a href="https://docs.aws.amazon.com/athena/latest/ug/partitions.html" target="_blank" rel="noopener">data partitioning</a> in the <em>Amazon Athena User Guide</em>.</p> 
<p>Now that the table and partitions are registered in the Data Catalog, you can query the inventory files with Amazon Athena.</p> 
<div class="hide-language"> 
 <pre><code class="lang-sql">SELECT * FROM crr_preexisting_demo where dt='2019-02-24-04-00';</code></pre> 
</div> 
<p>The results of the query are as follows.</p> 
<p><img class="alignnone size-full wp-image-6964" style="margin: 20px 0px 20px 0px;border: 1px solid #cccccc" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/TriggerCrossS3EMR7.png" alt="" width="800" height="186" /></p> 
<p>The query returns all rows in the S3 inventory for a specific delivery date. You’re now ready to launch an EMR cluster to copy in place the pre-existing and failed objects.</p> 
<p><strong>Note</strong>: If your goal is to fix FAILED objects, make sure that you correct what caused the failure (IAM permissions or S3 bucket policies) before proceeding to the next step.</p> 
<h2>Create an EMR cluster to copy objects</h2> 
<p>To parallelize the copy-in-place operations, run a Spark job on Amazon EMR. To facilitate EMR cluster creation and EMR step submission, we wrote a bash script (available in <a href="https://github.com/aws-samples/amazon-s3-crr-preexisting-objects/blob/master/launch_emr.sh" target="_blank" rel="noopener">this GitHub repository</a>).</p> 
<p>To run the script, clone the <a href="https://github.com/aws-samples/amazon-s3-crr-preexisting-objects">GitHub repo</a>. Then launch the EMR cluster as follows:</p> 
<div class="hide-language"> 
 <pre><code class="lang-code">$ git clone https://github.com/aws-samples/amazon-s3-crr-preexisting-objects
$ ./launch emr.sh</code></pre> 
</div> 
<p><strong>Note</strong>: Running the bash script results in AWS charges. By default, it creates two Amazon EC2 instances, one <em>m4.xlarge</em> and one <em>m4.2xlarge</em>. Auto-termination is enabled so when the cluster is finished with the in-place copies, it terminates.</p> 
<p>The script performs the following tasks:</p> 
<ol> 
 <li>Creates the default EMR roles (<code>EMR_EC2_DefaultRole</code> and&nbsp;<code>EMR_DefaultRole</code>).</li> 
 <li>Uploads the files used for bootstrap actions and steps to Amazon S3 (we use&nbsp;<code>crr-preexisting-demo-inventory</code> to store these files).</li> 
 <li>Creates an EMR cluster with Apache Spark installed using the <code>create-cluster</code></li> 
</ol> 
<p>After the cluster is provisioned:</p> 
<ol> 
 <li>A bootstrap action installs <em>boto3</em> and <em>awscli</em>.</li> 
 <li>Two steps execute, copying the Spark application to the master node and then running the application.</li> 
</ol> 
<p>The following are highlights from the Spark application. You can find the complete code for this example in the&nbsp;<a href="https://github.com/aws-samples/amazon-s3-crr-preexisting-objects" target="_blank" rel="noopener">amazon-s3-crr-preexisting-objects</a> repo on GitHub.</p> 
<p>Here we select records from the table registered with the AWS Glue Data Catalog, filtering for objects with a <code>replication_status</code> of <code>&quot;FAILED&quot;</code> or <code>“”</code>.</p> 
<div class="hide-language"> 
 <pre><code class="lang-python">query = &quot;&quot;&quot;
        SELECT bucket, key
        FROM {}
        WHERE dt = '{}'
        AND (replication_status = '&quot;&quot;'
        OR replication_status = '&quot;FAILED&quot;')
        &quot;&quot;&quot;.format(inventory_table, inventory_date)

print('Query: {}'.format(query))

crr_failed = spark.sql(query)</code></pre> 
</div> 
<p>We call the <code>copy_object</code> function for each <code>key</code> returned by the previous query.</p> 
<div class="hide-language"> 
 <pre><code class="lang-python">def copy_object(self, bucket, key, copy_acls):
        dest_bucket = self._s3.Bucket(bucket)
        dest_obj = dest_bucket.Object(key)

        src_bucket = self._s3.Bucket(bucket)
        src_obj = src_bucket.Object(key)

        # Get the S3 Object's Storage Class, Metadata, 
        # and Server Side Encryption
        storage_class, metadata, sse_type, last_modified = \
            self._get_object_attributes(src_obj)

        # Update the Metadata so the copy will work
        metadata['forcedreplication'] = runtime

        # Get and copy the current ACL
        if copy_acls:
            src_acl = src_obj.Acl()
            src_acl.load()
            dest_acl = {
                'Grants': src_acl.grants,
                'Owner': src_acl.owner
            }

        params = {
            'CopySource': {
                'Bucket': bucket,
                'Key': key
            },
            'MetadataDirective': 'REPLACE',
            'TaggingDirective': 'COPY',
            'Metadata': metadata,
            'StorageClass': storage_class
        }

        # Set Server Side Encryption
        if sse_type == 'AES256':
            params['ServerSideEncryption'] = 'AES256'
        elif sse_type == 'aws:kms':
            kms_key = src_obj.ssekms_key_id
            params['ServerSideEncryption'] = 'aws:kms'
            params['SSEKMSKeyId'] = kms_key

        # Copy the S3 Object over the top of itself, 
        # with the Storage Class, updated Metadata, 
        # and Server Side Encryption
        result = dest_obj.copy_from(**params)

        # Put the ACL back on the Object
        if copy_acls:
            dest_obj.Acl().put(AccessControlPolicy=dest_acl)

        return {
            'CopyInPlace': 'TRUE',
            'LastModified': str(result['CopyObjectResult']['LastModified'])
        }</code></pre> 
</div> 
<p><strong>Note</strong>: The Spark application adds a&nbsp;<code>forcedreplication</code> key to the objects’ metadata. It does this because Amazon S3 doesn’t allow you to copy in place without changing the object or its metadata.</p> 
<h2>Verify the success of the EMR job by running a query in Amazon Athena</h2> 
<p>The Spark application outputs its results to S3. You can create another external table with Amazon Athena and register it with the AWS Glue Data Catalog. You can then query the table with Athena to ensure that the copy-in-place operation was successful.</p> 
<div class="hide-language"> 
 <pre><code class="lang-sql">CREATE EXTERNAL TABLE IF NOT EXISTS
crr_preexisting_demo_results (
  `bucket` string,
  key string,
  replication_status string,
  last_modified string
)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY ','
  LINES TERMINATED BY '\n'
  STORED AS TEXTFILE
LOCATION 's3://crr-preexisting-demo-inventory/results';

SELECT * FROM crr_preexisting_demo_results;
</code></pre> 
</div> 
<p>The results appear as follows on the console.</p> 
<p><img class="alignnone size-full wp-image-6965" style="margin: 20px 0px 20px 0px;border: 1px solid #cccccc" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/TriggerCrossS3EMR8.png" alt="" width="800" height="174" /></p> 
<p>Although this shows that the copy-in-place operation was successful, CRR still needs to replicate the objects. Subsequent inventory files show the objects’ replication status as&nbsp;COMPLETED. You can also verify on the console that&nbsp;<code>preexisting-*.txt</code> and<em>&nbsp;</em><code>failed-*.txt</code> are COMPLETED.</p> 
<p><img class="alignnone size-full wp-image-6966" style="margin: 20px 0px 20px 0px;border: 1px solid #cccccc" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/TriggerCrossS3EMR9.png" alt="" width="274" height="94" /></p> 
<p>It is worth noting that because CRR requires versioned buckets, the copy-in-place operation produces another version of the objects. You can use <a href="https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html" target="_blank" rel="noopener">S3 lifecycle policies</a> to manage <em>noncurrent</em> versions.</p> 
<h2>Conclusion</h2> 
<p>In this post, we showed how to use Amazon S3 inventory, Amazon Athena, the AWS Glue Data Catalog, and Amazon EMR to perform copy-in-place operations on pre-existing and failed objects at scale.</p> 
<p><strong>Note</strong>: <a href="https://aws.amazon.com/about-aws/whats-new/2018/11/s3-batch-operations/" target="_blank" rel="noopener">Amazon S3 batch operations</a> is an alternative for copying objects. The difference is that S3 batch operations will not check each object’s existing properties and set object ACLs, storage class, and encryption on an object-by-object basis. For more information, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/user-guide/batch-ops.html" target="_blank" rel="noopener">Introduction to Amazon S3 Batch Operations</a> in the <em>Amazon S3 Console User Guide</em>.</p> 
<p>&nbsp;</p> 
<hr /> 
<h3>About the Authors</h3> 
<p><strong><img class="size-full wp-image-6973 alignleft" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/Michael.png" alt="" width="113" height="154" />Michael Sambol is a senior consultant at AWS. </strong>He holds an MS in computer science from Georgia Tech. Michael enjoys working out, playing tennis, traveling, and watching Western movies.<strong><br /> </strong></p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p><img class="size-full wp-image-6974 alignleft" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/29/Chauncy.png" alt="" width="113" height="151" /><strong>Chauncy McCaughey is a senior data architect at AWS.</strong> His current side project is using statistical analysis of driving habits and traffic patterns to understand how he always ends up in the slow lane.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p>