<p><img style="float: right;padding-left: 8px;padding-bottom: 8px" src="https://media.amazonwebservices.com/blog/2019/sb_ops_2.png" width="250" height="238" />AWS customers routinely store millions or billions of objects in individual <a href="https://aws.amazon.com/s3/" title="">Amazon Simple Storage Service (S3)</a> buckets, taking advantage of S3’s scale, durability, low cost, security, and storage options. These customers store images, videos, log files, backups, and other mission-critical data, and use S3 as a crucial part of their data storage strategy.</p> 
<p><span style="text-decoration: underline"><strong>Batch Operations</strong></span><br /> Today, I would like to tell you about Amazon S3 Batch Operations. You can use this new feature to easily process hundreds, millions, or billions of S3 objects in a simple and straightforward fashion. You can copy objects to another bucket, set tags or access control lists (ACLs), initiate a restore from Glacier, or invoke an <a href="https://aws.amazon.com/lambda/" title="">AWS Lambda</a> function on each one.</p> 
<p>This feature builds on S3’s existing support for inventory reports (read my <a href="https://aws.amazon.com/blogs/aws/s3-storage-management-update-analytics-object-tagging-inventory-and-metrics/">S3 Storage Management Update</a> post to learn more), and can use the reports or CSV files to drive your batch operations. You don’t have to write code, set up any server fleets, or figure out how to partition the work and distribute it to the fleet. Instead, you create a job in minutes with a couple of clicks, turn it loose, and sit back while S3 uses massive, behind-the-scenes parallelism to take care of the work. You can create, monitor, and manage your batch jobs using the <a href="https://s3.console.aws.amazon.com/s3/home">S3 Console</a>, the <a href="https://docs.aws.amazon.com/cli/latest/reference/s3api/index.html">S3 CLI</a>, or the <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html">S3 APIs</a>.</p> 
<p><span style="text-decoration: underline"><strong>A Quick Vocabulary Lesson</strong></span><br /> Before we get started and create a batch job, let’s review and introduce a couple of important terms:</p> 
<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html"><strong>Bucket</strong></a> – An S3 bucket holds a collection of any number of S3 <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html">objects</a>, with optional per-object <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html">versioning</a>.</p> 
<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html"><strong>Inventory Report</strong></a> – An S3 inventory report is generated each time a daily or weekly bucket inventory is run. A report can be configured to include all of the objects in a bucket, or to focus on a prefix-delimited subset.</p> 
<p><strong>Manifest</strong> – A list (either an Inventory Report, or a file in CSV format) that identifies the objects to be processed in the batch job.</p> 
<p><strong>Batch Action</strong> – The desired action on the objects described by a Manifest. Applying an action to an object constitutes an S3 Batch <strong>Task</strong>.</p> 
<p><strong>IAM Role</strong> – An IAM role that provides S3 with permission to read the objects in the inventory report, perform the desired actions, and to write the optional completion report. If you choose Invoke AWS Lambda function as your action, the function’s <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html">execution role</a> must grant permission to access the desired AWS services and resources.</p> 
<p><strong>Batch Job</strong> – References all of the items above. Each job has a status and a priority; higher priority (numerically) jobs take precedence over those with lower priority.</p> 
<p><span style="text-decoration: underline"><strong>Running a Batch Job</strong></span><br /> Ok, let’s use the <a href="https://console.aws.amazon.com/s3/">S3 Console</a> to create and run a batch job! In preparation for this blog post I enabled inventory reports for one of my S3 buckets (<strong>jbarr-batch-camera</strong>) earlier this week, with the reports routed to <strong>jbarr-batch-inventory</strong>:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_my_inv_1.png" width="900" height="366" /></p> 
<p>I select the desired inventory item, and click <strong>Create job from manifest</strong> to get started (I can also click <strong>Batch operations</strong> while browsing my list of buckets). All of the relevant information is already filled in, but I can choose an earlier version of the manifest if I want (this option is only applicable if the manifest is stored in a bucket that has versioning enabled). I click <strong>Next</strong> to proceed:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_create_p1_1.png" width="800" height="875" /></p> 
<p>I choose my operation (<strong>Replace all tags)</strong>, enter the options that are specific to it (I’ll review the other operations later), and click <strong>Next</strong>:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_create_p2_1.png" width="800" height="677" /></p> 
<p>I enter a name for my job, set its priority, and request a completion report that encompasses all tasks. Then I choose a bucket for the report and select an IAM Role that grants the necessary permissions (the console also displays a role policy and a trust policy that I can copy and use), and click <strong>Next</strong>:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_create_p3_1.png" width="800" height="1081" /></p> 
<p>Finally, I review my job, and click <strong>Create job</strong>:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_create_confirm_1.png" width="800" height="1236" /></p> 
<p>The job enters the <strong>Preparing</strong> state. S3 Batch Operations checks the manifest and does some other verification, and the job enters the <strong>Awaiting your confirmation</strong> state (this only happens when I use the console). I select it and click <strong>Confirm and run</strong>:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_job_conf_1.png" width="800" height="356" /></p> 
<p>I review the confirmation (not shown) to make sure that I understand the action to be performed, and click <strong>Run job</strong>. The job enters the <strong>Ready</strong> state, and starts to run shortly thereafter. When it is done it enters the <strong>Complete</strong> state:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_job_done_1.png" width="800" height="399" /></p> 
<p>If I was running a job that processed a substantially larger number of objects, I could refresh this page to monitor status. One important thing to know: After the first 1000 objects have been processed, S3 Batch Operations examines and monitors the overall failure rate, and will stop the job if the rate exceeds 50%.</p> 
<p>The completion report contains one line for each of my objects, and looks like this:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_report_1.png" width="800" height="400" /></p> 
<p><span style="text-decoration: underline"><strong>Other Built-In Batch Operations</strong></span><br /> I don’t have enough space to give you a full run-through of the other built-in batch operations. Here’s an overview:</p> 
<p>The <strong>PUT copy</strong> operation copies my objects, with control of the storage class, encryption, access control list, tags, and metadata:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_put_copy_1.png" width="700" height="646" /></p> 
<p>I can copy objects to the same bucket to change their encryption status. I can also copy them to another region, or to a bucket owned by another AWS account.</p> 
<p>The <strong>Replace Access Control List (ACL)</strong> operation does exactly that, with control over the permissions that are granted:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_replace_acl_1.png" width="700" height="643" /></p> 
<p>And the <strong>Restore</strong> operation initiates an object-level restore from the <a href="https://aws.amazon.com/s3/storage-classes/#Archive">Glacier</a> or <a href="https://aws.amazon.com/s3/storage-classes/#____">Glacier Deep Archive</a> storage class:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_restore_1.png" width="700" height="591" /></p> 
<p><span style="text-decoration: underline"><strong>Invoking AWS Lambda Functions</strong></span><br /> I have saved the most general option for last. I can invoke a Lambda function for each object, and that Lambda function can programmatically analyze and manipulate each object. The Execution Role for the function must trust S3 Batch Operations:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_lambda_role_trust_1.png" width="749" height="593" /></p> 
<p>Also, the Role for the Batch job must allow Lambda functions to be invoked.</p> 
<p>With the necessary roles in place, I can create a simple function that calls <a href="https://aws.amazon.com/rekognition/" title="">Amazon Rekognition</a> for each image:</p> 
<div class="hide-language"> 
 <pre class="unlimited-height-code"><code class="lang-python">import boto3
def lambda_handler(event, context):
    s3Client = boto3.client('s3')
    rekClient = boto3.client('rekognition')
    
    # Parse job parameters
    jobId = event['job']['id']
    invocationId = event['invocationId']
    invocationSchemaVersion = event['invocationSchemaVersion']

    # Process the task
    task = event['tasks'][0]
    taskId = task['taskId']
    s3Key = task['s3Key']
    s3VersionId = task['s3VersionId']
    s3BucketArn = task['s3BucketArn']
    s3Bucket = s3BucketArn.split(':')[-1]
    print('BatchProcessObject(' + s3Bucket + &quot;/&quot; + s3Key + ')')
    resp = rekClient.detect_labels(Image={'S3Object':{'Bucket' : s3Bucket, 'Name' : s3Key}}, MaxLabels=10, MinConfidence=85)
    
    l = [lb['Name'] for lb in resp['Labels']]
    print(s3Key + ' - Detected:' + str(sorted(l)))

    results = [{
        'taskId': taskId,
        'resultCode': 'Succeeded',
        'resultString': 'Succeeded'
    }]
    
    return {
        'invocationSchemaVersion': invocationSchemaVersion,
        'treatMissingKeysAs': 'PermanentFailure',
        'invocationId': invocationId,
        'results': results
    }</code></pre> 
</div> 
<p>With my function in place, I select <strong>Invoke AWS lambda function</strong> as my operation when I create my job, and choose my <code>BatchProcessObject</code> function:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_pick_fun_1.png" width="700" height="685" /></p> 
<p>Then I create and confirm my job as usual. The function will be invoked for each object, taking advantage of Lambda’s ability to scale and allowing this moderately-sized job to run to completion in less than a minute:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_lambda_scale_1.png" width="590" height="645" /></p> 
<p>I can find the “Detected” messages in the CloudWatch Logs Console:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_detected_1.png" width="900" height="436" /></p> 
<p>As you can see from my very simple example, the ability to easily run Lambda functions on large numbers of S3 objects opens the door to all sorts of interesting applications.</p> 
<p><span style="text-decoration: underline"><strong>Things to Know</strong></span><br /> I am looking forward to seeing and hearing about the use cases that you discover for S3 Batch Operations! Before I wrap up, here are some final thoughts:</p> 
<p><strong>Job Cloning</strong> – You can clone an existing job, fine-tune the parameters, and resubmit it as a fresh job. You can use this to re-run a failed job or to make any necessary adjustments.</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_clone_1.png" width="700" height="220" /></p> 
<p><strong>Programmatic Job Creation</strong> – You could attach a Lambda function to the bucket where you generate your inventory reports and create a fresh batch job each time a report arrives. Jobs that are created programmatically do not need to be confirmed, and are immediately ready to execute.</p> 
<p><strong>CSV Object Lists</strong> – If you need to process a subset of the objects in a bucket and cannot use a common prefix to identify them, you can create a CSV file and use it to drive your job. You could start from an inventory report and filter the objects based on name or by checking them against a database or other reference. For example, perhaps you use <a href="https://aws.amazon.com/comprehend/" title="">Amazon Comprehend</a> to perform sentiment analysis on all of your stored documents. You can process inventory reports to find documents that have not yet been analyzed and add them to a CSV file.</p> 
<p><strong>Job Priorities</strong> – You can have multiple jobs active at once in each AWS region. Your jobs with a higher priority take precedence, and can cause existing jobs to be paused momentarily. You can select an active job and click <strong>Update priority</strong> in order to make changes on the fly:</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2019/sb_update_pri_1.png" width="600" height="322" /></p> 
<p><span style="text-decoration: underline"><strong>Learn More</strong></span><br /> Here are some resources to help you learn more about S3 Batch Operations:</p> 
<p><strong>Documentation</strong> – Read about <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-basics.html">Creating a Job</a>, <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-operations.html">Batch Operations</a>, and <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-managing-jobs.html">Managing Batch Operations Jobs</a>.</p> 
<p><strong>Tutorial Videos</strong> – Check out the <a href="https://aws.amazon.com/s3/s3batchoperations-videos/">S3 Batch Operations Video Tutorials</a> to learn how to <a href="https://www.youtube.com/watch?v=hUv34voEftc">Create a Job</a>, <a href="https://www.youtube.com/watch?v=CuMDH6c0zm4">Manage and Track a Job</a>, and to <a href="https://www.youtube.com/watch?v=GrxlP39ye20">Grant Permissions</a>.</p> 
<p><span style="text-decoration: underline"><strong>Now Available</strong></span><br /> You can start using S3 Batch Operations in all commercial AWS regions except&nbsp;Asia Pacific (Osaka) today. S3 Batch Operations is also available in both of the AWS GovCloud (US) regions.</p> 
<p>— <a href="https://twitter.com/jeffbarr">Jeff</a>;</p><div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/AmazonWebServicesBlog?a=UKVPV3jqKZU:N6wQVCnSmoQ:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/AmazonWebServicesBlog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/AmazonWebServicesBlog?a=UKVPV3jqKZU:N6wQVCnSmoQ:dnMXMwOfBR0"><img src="http://feeds.feedburner.com/~ff/AmazonWebServicesBlog?d=dnMXMwOfBR0" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/AmazonWebServicesBlog?a=UKVPV3jqKZU:N6wQVCnSmoQ:7Q72WNTAKBA"><img src="http://feeds.feedburner.com/~ff/AmazonWebServicesBlog?d=7Q72WNTAKBA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/AmazonWebServicesBlog/~4/UKVPV3jqKZU" height="1" width="1" alt=""/>