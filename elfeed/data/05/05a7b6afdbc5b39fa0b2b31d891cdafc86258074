<p>Hashicorp Nomad 0.9 introduces new scheduling features that allow application owners and operators more fine-grained control over where to place their workloads. This post describes these features in more detail.</p>

<h2>Background</h2>

<p>The Nomad scheduler uses a bin packing algorithm to optimize resource utilization across a cluster. Application owners can use the <a href="https://www.nomadproject.io/docs/job-specification/constraint.html">constraint</a> stanza to limit the set of eligible nodes for placement. Apart from the constraint stanza, Nomad 0.8 and prior versions did not provide other fine grained control to users on where their workload would get placed in the cluster.</p>

<p>The new scheduling features in 0.9 provide additional flexibility in expressing placement preferences and allow operators to increase the failure tolerance of their workloads. These new features support use cases such as:</p>

<ul>
<li><p>Increasing the failure tolerance of a job by spreading its instances across multiple data centers or physical racks, via the <a href="https://www.nomadproject.io/docs/job-specification/spread.html">spread</a> stanza.</p></li>
<li><p>Targeting a specific class of nodes for specialized workloads via the new <a href="https://www.nomadproject.io/docs/job-specification/affinity.html">affinity</a> stanza.</p></li>
</ul>

<h2>Spread Stanza</h2>

<p>Nomad’s primary placement strategy is a <a href="https://www.nomadproject.io/docs/internals/scheduling/scheduling.html">bin packing</a> algorithm. Bin packing reduces overall infrastructure costs by optimizing placement on nodes that are running existing workloads. One downside to bin packing is that it could lead to situations where too many instances of the same workload end up in a single datacenter even if the job is specified to run in multiple data centers. If there is a catastrophic failure at the datacenter level, this can cause temporary outages until Nomad reschedules to another datacenter.</p>

<p>The <a href="https://www.nomadproject.io/docs/job-specification/spread.html">spread stanza</a> introduced in Nomad 0.9 solves this problem by allowing operators to distribute their workloads in a customized way based on <a href="https://www.nomadproject.io/docs/runtime/interpolation.html#node-variables-">attributes</a> and/or <a href="https://www.nomadproject.io/docs/configuration/client.html#meta">client metadata</a>. By using spread criteria in their job specification, Nomad job operators can ensure that failures across a domain such as datacenter or rack don&#39;t affect application availability.</p>

<p>The spread stanza can be specified at the job level as well as at the task group level. Job level spread criteria are inherited by all task groups in the job.</p>

<p>Example:
<code>hcl 
job &quot;docs&quot; {
    datacenters = [“us-east1”, “us-east2”]
    #Spread allocations over all datacenter
    spread {
        attribute =  &quot;${node.datacenter}&quot;
    }
    group &quot;test&quot; {
        count = 10
        #Spread allocations over each rack based on desired percentage
        spread {
            attribute =  &quot;${meta.rack}&quot;
            target &quot;r1&quot; {
                percent =  60
            }
            target &quot;r2&quot; {
                percent =  40
            }
        }
    }
}
</code></p>

<p>In the above example, the job has a spread stanza based on the datacenter of the node. By default, Nomad uses a uniform spread strategy when a spread stanza does not specify specific target percentages. Nomad will prefer that each datacenter runs 5 instances of the job.</p>

<p>Spread stanzas can also have specific target percentages. The task group “test” in the above example specifies different target percentages for <code>&quot;r1”</code> and <code>“r2”</code>. Nomad will ensure that 60% of the instances schedule onto nodes in <code>“r1”</code>, and 40% in nodes in <code>“r2”</code>.</p>

<p>The spread stanza also works when targets are partially specified. In the same example, if we removed the target for <code>&quot;r2”</code>, and there were more than two racks, Nomad will schedule 50% of instances in <code>“r1”</code> and evenly spread the other remaining instances across all other racks.</p>

<p>Spread criteria are also treated as a soft preference by the Nomad scheduler. If no nodes match a given spread criteria, placement is still successful as long as constraints and resource requirements can be satisfied</p>

<p>By using spread criteria in their job specification, Nomad job operators can ensure that failures across a domain such as datacenter or rack don&#39;t affect application availability. For more details and examples, refer to our <a href="https://www.nomadproject.io/docs/job-specification/spread.html">spread documentation.</a></p>

<h2>Affinity Stanza</h2>

<p>As mentioned above, previous versions of Nomad have a <a href="https://www.nomadproject.io/docs/job-specification/constraint.html">constraint</a> stanza which strictly filters where jobs are run based on attributes and client metadata. If no nodes are found to match, the placement does not succeed.</p>

<p>The <a href="https://www.nomadproject.io/docs/job-specification/affinity.html">affinity</a> stanza in Nomad 0.9 allows operators to express placement preferences for their jobs on particular types of nodes.The affinity stanza acts like a &quot;soft constraint.&quot; Nomad will attempt to match the desired affinity, but placement will succeed even if no nodes match the desired criteria.</p>

<p>When scoring nodes for placement, Nomad will take any matching affinities into account so that nodes that match preferred criteria are scored higher. Scores from affinities are combined with other scoring factors such as bin packing.</p>

<p>Similar to the constraint stanza, the affinity stanza can be specified at the job level as well as at the task group and task levels. Job level affinities are inherited by all task groups in the job. Task level affinities are combined together with task group level affinities.</p>

<p>Example:
<code>hcl
job &quot;docs&quot; {
    #Prefer m4.xlarge nodes
    affinity {
        attribute = &quot;${attr.platform.aws.instance-type}&quot;
        value = &quot;m4.xlarge&quot;
        weight =  100
    }
    group &quot;example&quot; {
        #Prefer the &quot;r1&quot; rack
        affinity {
            attribute =  &quot;${meta.rack}&quot;
            value =  &quot;r1&quot;
            weight =  50
        }
        task &quot;server&quot; {
            ..
        }
    }
</code></p>

<p>In the above example, the job has an affinity for <code>&quot;m4.xlarge&quot;</code> node. This affinity will apply to all task groups in the job. The task group also has an affinity for a specific rack  <code>&quot;r1&quot;</code>. Nomad will add additional boosting factors to the score of nodes that match these two affinities so that they are preferred for placement. However if no nodes are found that match these affinities, the placement still succeeds.</p>

<p>Negative weights act like anti-affinities, and encourage the matching nodes to be avoided for placement by Nomad.</p>

<p>The affinity stanza is useful for use cases like preferring a specific class of nodes for workloads with specialized requirements. For more details and examples, refer to our <a href="https://www.nomadproject.io/docs/job-specification/affinity.html">affinity documentation.</a></p>

<h2>Conclusion</h2>

<p>Nomad 0.9 introduces two new stanzas, spread and affinity, that allow for advanced placement strategies and increasing failure tolerance. With these features, Nomad 0.9 provides more fine grained control over workload placement to operators and job specification authors.</p>
