<p>With <a href="https://www.hashicorp.com/blog/announcing-terraform-0-12">Terraform 0.12 generally available</a>, new configuration language improvements allow additional templating of Kubernetes resources. In this post, we will demonstrate how to use Terraform 0.12, the <a href="https://www.terraform.io/docs/providers/kubernetes/">Kubernetes Provider</a>, and the <a href="https://www.terraform.io/docs/providers/helm/">Helm provider</a> for configuration and deployment of Kubernetes resources.</p>

<p>The following examples demonstrate the use of Terraform providers to deploy additional services and functions for supporting applications:</p>

<ul>
<li>ExternalDNS deployment, to set up DNS aliases for services or ingresses.</li>
<li>Fluentd daemonset, for sending application logs.</li>
<li>Consul Helm chart, for service mesh and application configuration.</li>
</ul>

<p>Deployment of these services happens after creating the infrastructure and Kubernetes cluster with a Terraform <a href="https://www.terraform.io/docs/providers/type/major-index.html">cloud provider</a>.</p>

<h2>ExternalDNS Deployment</h2>

<p>A <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Kubernetes deployment</a> maintains the desired number of application pods. In this example, we create a Kubernetes deployment with Terraform that will interpolate identifiers and attributes from resources created by the cloud provider. This alleviates the need for separate or additional automation to retrieve attributes such as hosted zone identifiers, domain names, and CIDR blocks.</p>

<p>We can use <a href="https://github.com/kubernetes-incubator/external-dns">ExternalDNS</a> to create a DNS record for a service upon creation or update. ExternalDNS runs in Kubernetes as a deployment. First, we translate the <a href="https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/aws.md">Kubernetes deployment configuration file for ExternalDNS</a> to Terraformâ€™s configuration language (called HCL). This allows Terraform to display the differences in each section as changes are applied. The code below shows the Terraform <a href="https://www.terraform.io/docs/providers/kubernetes/r/deployment.html">kubernetes_deployment</a> resource to create ExternalDNS.</p>

<p>```hcl
locals {
  name = &quot;external-dns&quot;
}</p>

<p>resource &quot;aws<em>route53</em>zone&quot; &quot;dev&quot; {
  name = &quot;dev.${var.domain}&quot;
}</p>

<p>resource &quot;kubernetes<em>deployment&quot; &quot;external</em>dns&quot; {
  metadata {
    name      = local.name
    namespace = var.namespace
  }</p>

<p>spec {
    selector {
      match_labels = {
        app = local.name
      }
    }</p>

<pre><code>template {
  metadata {
    labels = {
      app = local.name
    }
  }

  spec {
    container {
      name  = local.name
      image = var.image
      args = concat([
        &quot;--source=service&quot;,
        &quot;--source=ingress&quot;,
        &quot;--domain-filter=${aws_route53_zone.dev.name}&quot;,
        &quot;--provider=${var.cloud_provider}&quot;,
        &quot;--policy=upsert-only&quot;,
        &quot;--registry=txt&quot;,
        &quot;--txt-owner-id=${aws_route53_zone.dev.zone_id}&quot;
      ], var.other_provider_options)
    }

    service_account_name = local.name
  }
}

strategy {
  type = &quot;Recreate&quot;
}
</code></pre>

<p>}
}
```</p>

<p>Note that we use the Terraform 0.12 first class expressions, such as <code>var.namespace</code> or <code>local.name</code>, without the need for variable interpolation syntax. Furthermore, we <a href="https://www.terraform.io/docs/configuration/expressions.html#lt-resource-type-gt-lt-name-gt-">resource reference</a> the hosted zone resource we created with the <code>aws_route53_zone</code>. The dynamic reference to the AWS resource removes our need to separately extract and inject the attributes into a Kubernetes manifest.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/zHc2dUBiye4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h2>Kubernetes DaemonSets</h2>

<p>To collect application logs, we can deploy Fluentd as a Kubernetes daemonset. <a href="https://www.fluentd.org/">Fluentd</a> collects, structures, and forwards logs to a logging server for aggregation. Each Kubernetes node must have an instance of Fluentd. A <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">Kubernetes daemonset</a> ensures a pod is running on each node. In the following
example, we configure the Fluentd daemonset to <a href="https://github.com/fluent/fluentd-kubernetes-daemonset">use Elasticsearch</a> as the logging server.</p>

<p>Configuring Fluentd to target a logging server requires a number of environment variables, including ports, hostnames, and usernames. In versions of Terraform prior to 0.12, we duplicated blocks such as <code>volume</code> or <code>env</code> and added different parameters to each one. The excerpt below demonstrates the Terraform version &lt;0.12 configuration for the Fluentd daemonset.</p>

<p>```hcl
resource &quot;kubernetes_daemonset&quot; &quot;fluentd&quot; {
  metadata {
    name = &quot;fluentd&quot;
  }</p>

<p>spec {
    template {
      spec {
        container {
          name  = &quot;fluentd&quot;
          image = &quot;fluent/fluentd-kubernetes-daemonset:elasticsearch&quot;</p>

<pre><code>      env {
        name  = &quot;FLUENT_ELASTICSEARCH_HOST&quot;
        value = &quot;elasticsearch-logging&quot;
      }

      env {
        name  = &quot;FLUENT_ELASTICSEARCH_PORT&quot;
        value = &quot;9200&quot;
      }

      env {
        name  = &quot;FLUENT_ELASTICSEARCH_SCHEME&quot;
        value = &quot;http&quot;
      }

      env {
        name  = &quot;FLUENT_ELASTICSEARCH_USER&quot;
        value = &quot;elastic&quot;
      }

      env {
        name  = &quot;FLUENT_ELASTICSEARCH_PASSWORD&quot;
        value = &quot;changeme&quot;
      }
    }
  }
}
</code></pre>

<p>}
}
```</p>

<p>Using Terraform 0.12 <a href="https://www.terraform.io/docs/configuration/expressions.html#dynamic-blocks">dynamic blocks</a>, we can specify a list of environment variables and use a <code>for_each</code> loop to create each <code>env</code> child block in the daemonset.</p>

<p>```hcl
locals {
  name = &quot;fluentd&quot;</p>

<p>labels = {
    k8s-app = &quot;fluentd-logging&quot;
    version = &quot;v1&quot;
  }</p>

<p>env_variables = {
    &quot;HOST&quot; : &quot;elasticsearch-logging&quot;,
    &quot;PORT&quot; : var.port,
    &quot;SCHEME&quot; : &quot;http&quot;,
    &quot;USER&quot; : var.user,
    &quot;PASSWORD&quot; : var.password
  }
}</p>

<p>resource &quot;kubernetes_daemonset&quot; &quot;fluentd&quot; {
  metadata {
    name      = local.name
    namespace = var.namespace</p>

<pre><code>labels = local.labels
</code></pre>

<p>}</p>

<p>spec {
    selector {
      match_labels = {
        k8s-app = local.labels.k8s-app
      }
    }</p>

<pre><code>template {
  metadata {
    labels = local.labels
  }

  spec {
    volume {
      name = &quot;varlog&quot;

      host_path {
        path = &quot;/var/log&quot;
      }
    }

    volume {
      name = &quot;varlibdockercontainers&quot;

      host_path {
        path = &quot;/var/lib/docker/containers&quot;
      }
    }

    container {
      name  = local.name
      image = var.image

      dynamic &quot;env&quot; {
        for_each = local.env_variables
        content {
          name  = &quot;FLUENT_ELASTICSEARCH_${env.key}&quot;
          value = env.value
        }
      }

      resources {
        limits {
          memory = &quot;200Mi&quot;
        }

        requests {
          cpu    = &quot;100m&quot;
          memory = &quot;200Mi&quot;
        }
      }

      volume_mount {
        name       = &quot;varlog&quot;
        mount_path = &quot;/var/log&quot;
      }

      volume_mount {
        name       = &quot;varlibdockercontainers&quot;
        read_only  = true
        mount_path = &quot;/var/lib/docker/containers&quot;
      }
    }

    termination_grace_period_seconds = 30
    service_account_name             = local.name
  }
}
</code></pre>

<p>}
}
```</p>

<p>In this example, we specify a map with a key and value for each environment variable. The <code>dynamic &quot;env&quot;</code> block iterates over entry in the map, retrieves the key and value, and creates an <code>env</code> child block. This minimizes duplication in configuration and allows any number of environment variables to be added or removed.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/z7gwaPaySh0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h2>Managing Helm Charts via Terraform</h2>

<p>For services packaged with <a href="https://helm.sh/">Helm</a>, we can also use Terraform to deploy charts and run tests.  Helm provides application definitions in the form of <a href="https://helm.sh/docs/developing_charts/">charts</a>. Services or applications often have official charts for streamlining deployment. For example, we might want to use <a href="https://www.consul.io/">Consul</a>, a service mesh that provides a key-value store, to connect applications and manage configuration in our Kubernetes cluster.</p>

<p>We can use the official <a href="https://github.com/hashicorp/consul-helm/">Consul Helm chart</a>, which packages the necessary Consul application definitions for deployment. When using Helm directly, we would first deploy a component called Tiller for version 2 of Helm. Then, we would store the Consul chart locally, deploy the chart with <code>helm install</code>, and test the deployment with <code>helm test</code>.</p>

<p>When using Terraform Helm provider, the provider will handle deployment of Tiller, installation of a Consul cluster via the chart, and triggering of acceptance tests. First, we include an option to <code>install_tiller</code> with the Helm provider.</p>

<p><code>hcl
provider &quot;helm&quot; {
  version        = &quot;~&gt; 0.9&quot;
  install_tiller = true
}
</code></p>

<p>Next, we use the Terraform <a href="https://www.terraform.io/docs/providers/helm/release.html">helm_release</a> resource to deploy the chart. We pass the variables to the Helm chart with <code>set</code> blocks. We also include a <a href="https://www.terraform.io/docs/provisioners/index.html">provisioner</a> to run a set of acceptance tests after deployment, using <code>helm test</code>. The acceptance tests confirm if Consul is ready for use.</p>

<p>```hcl
resource &quot;helm_release&quot; &quot;consul&quot; {
  name      = var.name
  chart     = &quot;${path.module}/consul-helm&quot;
  namespace = var.namespace</p>

<p>set {
    name  = &quot;server.replicas&quot;
    value = var.replicas
  }</p>

<p>set {
    name  = &quot;server.bootstrapExpect&quot;
    value = var.replicas
  }</p>

<p>set {
    name  = &quot;server.connect&quot;
    value = true
  }</p>

<p>provisioner &quot;local-exec&quot; {
    command = &quot;helm test ${var.name}&quot;
  }
}
```</p>

<p>When we run <code>terraform apply</code>, Terraform deploys the Helm release and runs the tests. By using Terraform to deploy the Helm release, we can pass attributes from infrastructure resources to the curated application definition in Helm and run available acceptance tests in a single, common workflow.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/d9NTBb6zByE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h2>Conclusion</h2>

<p>We can use Terraform to not only manage and create Kubernetes clusters but also create resources on clusters with the Kubernetes API or Helm. We examined how to interpolate resource identifiers and attributes from infrastructure resources into Kubernetes services, such as ExternalDNS. Furthermore, we used improvements in Terraform 0.12 to minimize configuration and deploy a Fluentd daemonset. Finally, we deployed and tested Consul using the Terraform Helm provider.</p>

<p>Leveraging this combination of providers allows users to seamlessly pass attributes from infrastructure to Kubernetes clusters and minimize additional automation to retrieve them. For more information about Terraform 0.12 and its improvements, see our <a href="https://www.hashicorp.com/blog/announcing-terraform-0-12">blog announcing Terraform 0.12</a>. To learn more about providers, see the <a href="https://www.terraform.io/docs/providers/kubernetes/index.html">Kubernetes provider reference</a> and the <a href="https://www.terraform.io/docs/providers/helm/index.html">Helm provider reference</a>.</p>
