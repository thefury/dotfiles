<p>You generally write unit tests for your code, but do you also test your data? Incorrect or malformed data can have a large impact on production systems. Examples of data quality issues are:</p> 
<ul> 
 <li>Missing values can lead to failures in production system that require non-null values (NullPointerException).</li> 
 <li>Changes in the distribution of data can lead to unexpected outputs of machine learning models.</li> 
 <li>Aggregations of incorrect data can lead to wrong business decisions.</li> 
</ul> 
<p>In this blog post, we introduce Deequ, an open source tool developed and used at Amazon. Deequ allows you to calculate data quality metrics on your dataset, define and verify data quality constraints, and be informed about changes in the data distribution. Instead of implementing checks and verification algorithms on your own, you can focus on describing how your data should look. Deequ supports you by suggesting checks for you. Deequ is implemented on top of <u><a href="https://spark.apache.org/" target="_blank" rel="noopener">Apache Spark</a></u> and is designed to scale with large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse.</p> 
<h2><strong>Deequ at Amazon</strong></h2> 
<p>Deequ is being used internally at Amazon for verifying the quality of many large production datasets. Dataset producers can add and edit data quality constraints. The system computes data quality metrics on a regular basis (with every new version of a dataset), verifies constraints defined by dataset producers, and publishes datasets to consumers in case of success. In error cases, dataset publication can be stopped, and producers are notified to take action. Data quality issues do not propagate to consumer data pipelines, reducing their blast radius.</p> 
<h2><strong>Overview of Deequ</strong></h2> 
<p>To use Deequ, let’s look at its main components (also shown in Figure 1).</p> 
<ul> 
 <li><strong>Metrics Computation</strong> — Deequ computes data quality metrics, that is, statistics such as completeness, maximum, or correlation. Deequ uses Spark to read from sources such as Amazon S3, and to compute metrics through an optimized set of aggregation queries. You have direct access to the raw metrics computed on the data.</li> 
 <li><strong>Constraint Verification</strong> — As a user, you focus on defining a set of data quality constraints to be verified. Deequ takes care of deriving the required set of metrics to be computed on the data. Deequ generates a data quality report, which contains the result of the constraint verification.</li> 
 <li><strong>Constraint Suggestion</strong> — You can choose to define your own custom data quality constraints, or use the automated constraint suggestion methods that profile the data to infer useful constraints.</li> 
</ul> 
<p><img class="alignnone size-full wp-image-6902" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/10/DataDeequ1.png" alt="" width="800" height="407" /></p> 
<p>Figure 1: Overview of Deequ components</p> 
<h2><strong>Setup: Launch the Spark cluster</strong></h2> 
<p>This section shows the steps to use Deequ on your own data. First, set up Spark and Deequ on an Amazon EMR cluster. Then, load a sample dataset provided by AWS, run some analysis, and then run data tests.</p> 
<p>Deequ is built on top of Apache Spark to support fast, distributed calculations on large datasets. Deequ depends on Spark version 2.2.0 or later. As a first step, <u><a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-launch.html" target="_blank" rel="noopener">create a cluster with Spark on Amazon EMR.</a></u> Amazon EMR takes care of the configuration of Spark for you. Also, you canuse the EMR File System (EMRFS) to directly access data in Amazon S3. For testing, you can also install Spark on a single machine <u><a href="https://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">in standalone mode</a></u>.</p> 
<p><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-ssh.html" target="_blank" rel="noopener">Connect to the Amazon EMR master node using SSH</a>. Load the latest Deequ JAR from <a href="https://mvnrepository.com/artifact/com.amazon.deequ/deequ" target="_blank" rel="noopener"><u>Maven Repository</u></a>. To load the JAR of version 1.0.1, use the following:</p> 
<p><tt>wget http://repo1.maven.org/maven2/com/amazon/deequ/deequ/1.0.1/deequ-1.0.1.jar</tt></p> 
<p>Launch Spark Shell and use the spark.jars argument for referencing the Deequ JAR file:</p> 
<p><tt>spark-shell --conf spark.jars=deequ-1.0.1.jar</tt></p> 
<p>For more information about how to set up Spark, see the <u><a href="https://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="noopener">Spark Quick Start guide</a>,</u> and the overview of <u><a href="https://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">Spark configuration</a></u> options.</p> 
<h3><strong>Load data</strong></h3> 
<p>As a running example, we use <u><a href="https://s3.amazonaws.com/amazon-reviews-pds/readme.html" target="_blank" rel="noopener">a customer review dataset provided by Amazon</a></u> on Amazon S3. Let’s load the dataset containing reviews for the category “Electronics” in Spark. Make sure to enter the code in the Spark shell:</p> 
<p><tt>val dataset = spark.read.parquet(&quot;s3://amazon-reviews-pds/parquet/product_category=Electronics/&quot;)</tt></p> 
<p>You can see the following selected attributes if you run <tt>dataset.printSchema()</tt> in the Spark shell:</p> 
<p><tt>root<br /> |-- marketplace: string (nullable = true)<br /> |-- customer_id: string (nullable = true)<br /> |-- review_id: string (nullable = true)<br /> |-- product_title: string (nullable = true)<br /> |-- star_rating: integer (nullable = true)<br /> |-- helpful_votes: integer (nullable = true)<br /> |-- total_votes: integer (nullable = true)<br /> |-- vine: string (nullable = true)<br /> |-- year: integer (nullable = true)</tt></p> 
<h3><strong>Data analysis</strong></h3> 
<p>Before we define checks on the data, we want to calculate some statistics on the dataset; we call them <em>metrics</em>. Deequ supports the following metrics (they are defined in <a href="https://github.com/awslabs/deequ/tree/master/src/main/scala/com/amazon/deequ/analyzers" target="_blank" rel="noopener">this Deequ package</a>):</p> 
<table border="1" cellpadding="10"> 
 <tbody> 
  <tr style="background-color: #000000"> 
   <td width="130"> <p style="text-align: center"><span style="color: #ffffff"><strong>Metric</strong></span></p> </td> 
   <td style="text-align: center" width="235"><span style="color: #ffffff"><strong>Description</strong></span></td> 
   <td width="325"> <p style="text-align: center"><span style="color: #ffffff"><strong>Usage Example</strong></span></p> </td> 
  </tr> 
  <tr> 
   <td width="130"><strong>ApproxCountDistinct</strong></td> 
   <td width="235">Approximate number of distinct value, computed with HyperLogLogPlusPlus sketches.</td> 
   <td width="325"><tt>ApproxCountDistinct(&quot;review_id&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>ApproxQuantile</strong></td> 
   <td width="235">Approximate quantile of a distribution.</td> 
   <td width="325"><tt>ApproxQuantile(&quot;star_rating&quot;, quantile = 0.5)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>ApproxQuantiles</strong></td> 
   <td width="235">Approximate quantiles of a distribution.</td> 
   <td width="325"><tt>ApproxQuantiles(&quot;star_rating&quot;, quantiles = Seq(0.1, 0.5, 0.9))</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Completeness</strong></td> 
   <td width="235">Fraction of non-null values in a column.</td> 
   <td width="325"><tt>Completeness(&quot;review_id&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Compliance</strong></td> 
   <td width="235">Fraction of rows that comply with the given column constraint.</td> 
   <td width="325"><tt>Compliance(&quot;top star_rating&quot;, &quot;star_rating &gt;= 4.0&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Correlation</strong></td> 
   <td width="235">Pearson correlation coefficient, measures the linear correlation between two columns. The result is in the range [-1, 1], where 1 means positive linear correlation, -1 means negative linear correlation, and 0 means no correlation.</td> 
   <td width="325"><tt>Correlation(&quot;total_votes&quot;, &quot;star_rating&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>CountDistinct</strong></td> 
   <td width="235">Number of distinct values.</td> 
   <td width="325"><tt>CountDistinct(&quot;review_id&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>DataType</strong></td> 
   <td width="235">Distribution of data types such as Boolean, Fractional, Integral, and String. The resulting histogram allows filtering by relative or absolute fractions.</td> 
   <td width="325"><tt>DataType(&quot;year&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Distinctness</strong></td> 
   <td width="235">Fraction of distinct values of a column over the number of all values of a column. Distinct values occur at least once. Example: [a, a, b] contains two distinct values a and b, so distinctness is 2/3.</td> 
   <td width="325"><tt>Distinctness(&quot;review_id&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Entropy</strong></td> 
   <td width="235">Entropy is a measure of the level of information contained in an event (value in a column) when considering all possible events (values in a column). It is measured in nats (natural units of information). Entropy is estimated using observed value counts as the negative sum of (value_count/total_count) * log(value_count/total_count). Example: [a, b, b, c, c] has three distinct values with counts [1, 2, 2]. Entropy is then (-1/5*log(1/5)-2/5*log(2/5)-2/5*log(2/5)) = 1.055.</td> 
   <td width="325"><tt>Entropy(&quot;star_rating&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Maximum</strong></td> 
   <td width="235">Maximum value.</td> 
   <td width="325"><tt>Maximum(&quot;star_rating&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Mean</strong></td> 
   <td width="235">Mean value; null values are excluded.</td> 
   <td width="325"><tt>Mean(&quot;star_rating&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Minimum</strong></td> 
   <td width="235">Minimum value.</td> 
   <td width="325"><tt>Minimum(&quot;star_rating&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>MutualInformation</strong></td> 
   <td width="235">Mutual information describes how much information about one column (one random variable) can be inferred from another column (another random variable). If the two columns are independent, mutual information is zero. If one column is a function of the other column, mutual information is the entropy of the column. Mutual information is symmetric and nonnegative.</td> 
   <td width="325"><tt>MutualInformation(Seq(&quot;total_votes&quot;, &quot;star_rating&quot;))</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>PatternMatch</strong></td> 
   <td width="235">Fraction of rows that comply with a given regular experssion.</td> 
   <td width="325"><tt>PatternMatch(&quot;marketplace&quot;, pattern = raw&quot;\w{2}&quot;.r)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Size</strong></td> 
   <td width="235">Number of rows in a DataFrame.</td> 
   <td width="325"><tt>Size()</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Sum</strong></td> 
   <td width="235">Sum of all values of a column.</td> 
   <td width="325"><tt>Sum(&quot;total_votes&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>UniqueValueRatio</strong></td> 
   <td width="235">Fraction of unique values over the number of all distinct values of a column. Unique values occur exactly once; distinct values occur at least once.&nbsp;Example: [a, a, b] contains one unique value b, and&nbsp;two distinct values a and b,&nbsp;so the unique value ratio is 1/2.</td> 
   <td width="325"><tt>UniqueValueRatio(&quot;star_rating&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td width="130"><strong>Uniqueness</strong></td> 
   <td width="235">Fraction of unique values over the number of all values of a column.&nbsp;Unique values occur exactly once.&nbsp;Example: [a, a, b] contains one unique value b, so uniqueness is 1/3.</td> 
   <td width="325"><tt>Uniqueness(&quot;star_rating&quot;)</tt></td> 
  </tr> 
 </tbody> 
</table> 
<p>In the following example, we show how to use the <u><a href="https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/analyzers/runners/AnalysisRunner.scala" target="_blank" rel="noopener">AnalysisRunner</a></u> to define the metrics you are interested in. You can run the following code in the Spark shell by either just pasting it in the shell or by saving it in a local file on the master node and loading it in the Spark shell with the following command:</p> 
<p><tt>:load PATH_TO_FILE</tt></p> 
<div class="hide-language"> 
 <pre><code class="lang-scala">import com.amazon.deequ.analyzers.runners.{AnalysisRunner, AnalyzerContext}
import com.amazon.deequ.analyzers.runners.AnalyzerContext.successMetricsAsDataFrame
import com.amazon.deequ.analyzers.{Compliance, Correlation, Size, Completeness, Mean, ApproxCountDistinct}

val analysisResult: AnalyzerContext = { AnalysisRunner
  // data to run the analysis on
  .onData(dataset)
  // define analyzers that compute metrics
  .addAnalyzer(Size())
  .addAnalyzer(Completeness(&quot;review_id&quot;))
  .addAnalyzer(ApproxCountDistinct(&quot;review_id&quot;))
  .addAnalyzer(Mean(&quot;star_rating&quot;))
  .addAnalyzer(Compliance(&quot;top star_rating&quot;, &quot;star_rating &gt;= 4.0&quot;))
  .addAnalyzer(Correlation(&quot;total_votes&quot;, &quot;star_rating&quot;))
  .addAnalyzer(Correlation(&quot;total_votes&quot;, &quot;helpful_votes&quot;))
  // compute metrics
  .run()
}

// retrieve successfully computed metrics as a Spark data frame
val metrics = successMetricsAsDataFrame(spark, analysisResult)</code></pre> 
</div> 
<p>The resulting data frame contains the calculated metrics (call <tt>metrics.show()</tt> in the Spark shell):</p> 
<table border="1" cellpadding="10"> 
 <tbody> 
  <tr style="background-color: #000000"> 
   <td style="text-align: center"><span style="color: #ffffff"><strong>name</strong></span></td> 
   <td style="text-align: center"><span style="color: #ffffff"><strong>instance</strong></span></td> 
   <td style="text-align: center"><span style="color: #ffffff"><strong>value</strong></span></td> 
  </tr> 
  <tr> 
   <td>ApproxCountDistinct</td> 
   <td>review_id</td> 
   <td>3010972</td> 
  </tr> 
  <tr> 
   <td>Completeness</td> 
   <td>review_id</td> 
   <td>1</td> 
  </tr> 
  <tr> 
   <td>Compliance</td> 
   <td>top star_rating</td> 
   <td>0.74941</td> 
  </tr> 
  <tr> 
   <td>Correlation</td> 
   <td>helpful_votes,total_votes</td> 
   <td>0.99365</td> 
  </tr> 
  <tr> 
   <td>Correlation</td> 
   <td>total_votes,star_rating</td> 
   <td>-0.03451</td> 
  </tr> 
  <tr> 
   <td>Mean</td> 
   <td>star_rating</td> 
   <td>4.03614</td> 
  </tr> 
  <tr> 
   <td>Size</td> 
   <td>*</td> 
   <td>3120938</td> 
  </tr> 
 </tbody> 
</table> 
<p>We can learn that:</p> 
<ul> 
 <li><tt>review_id</tt> has no missing values and approximately 3,010,972 unique values.</li> 
 <li>74.9 % of reviews have a <tt>star_rating</tt> of 4 or higher.</li> 
 <li><tt>total_votes</tt> and <tt>star_rating</tt> are not correlated.</li> 
 <li><tt>helpful_votes</tt> and <tt>total_votes</tt> are strongly correlated.</li> 
 <li>The average <tt>star_rating</tt> is 4.0.</li> 
 <li>The dataset contains 3,120,938 reviews.</li> 
</ul> 
<h3><strong>Define and Run Tests for Data</strong></h3> 
<p>After analyzing and understanding the data, we want to verify that the properties we have derived also hold for new versions of the dataset. By defining assertions on the data distribution as part of a data pipeline, we can ensure that every processed dataset is of high quality, and that any application consuming the data can rely on it.</p> 
<p>For writing tests on data, we start with the <u><a href="https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/VerificationSuite.scala" target="_blank" rel="noopener">VerificationSuite</a></u> and add <u><a href="https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/checks/Check.scala" target="_blank" rel="noopener">Checks</a></u> on attributes of the data. In this example, we test for the following properties of our data:</p> 
<ul> 
 <li>There are at least 3 million rows in total.</li> 
 <li><tt>review_id</tt> is never NULL.</li> 
 <li><tt>review_id</tt> is unique.</li> 
 <li><tt>star_rating</tt> has a minimum of 1.0 and a maximum of 5.0.</li> 
 <li><tt>marketplace</tt> only contains “US”, “UK”, “DE”, “JP”, or “FR”.</li> 
 <li><tt>year</tt> does not contain negative values.</li> 
</ul> 
<p>This is the code that reflects the previous statements. For information about all available checks, see <u><a href="https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/checks/Check.scala" target="_blank" rel="noopener">this GitHub repository</a></u>. You can run this directly in the Spark shell as previously explained:</p> 
<div class="hide-language"> 
 <pre><code class="lang-scala">import com.amazon.deequ.{VerificationSuite, VerificationResult}
import com.amazon.deequ.VerificationResult.checkResultsAsDataFrame
import com.amazon.deequ.checks.{Check, CheckLevel}

val verificationResult: VerificationResult = { VerificationSuite()
  // data to run the verification on
  .onData(dataset)
  // define a data quality check
  .addCheck(
    Check(CheckLevel.Error, &quot;Review Check&quot;) 
      .hasSize(_ &gt;= 3000000) // at least 3 million rows
      .hasMin(&quot;star_rating&quot;, _ == 1.0) // min is 1.0
      .hasMax(&quot;star_rating&quot;, _ == 5.0) // max is 5.0
      .isComplete(&quot;review_id&quot;) // should never be NULL
      .isUnique(&quot;review_id&quot;) // should not contain duplicates
      .isComplete(&quot;marketplace&quot;) // should never be NULL
      // contains only the listed values
      .isContainedIn(&quot;marketplace&quot;, Array(&quot;US&quot;, &quot;UK&quot;, &quot;DE&quot;, &quot;JP&quot;, &quot;FR&quot;))
      .isNonNegative(&quot;year&quot;)) // should not contain negative values
  // compute metrics and verify check conditions
  .run()
}

// convert check results to a Spark data frame
val resultDataFrame = checkResultsAsDataFrame(spark, verificationResult)</code></pre> 
</div> 
<p>After calling <tt>run</tt>, Deequ translates your test description into a series of Spark jobs, which are executed to compute metrics on the data. Afterwards, it invokes your assertion functions (e.g., _ == 1.0 for the minimum star-rating check) on these metrics to see if the constraints hold on the data.</p> 
<p>Call <tt>resultDataFrame.show(truncate=false)</tt> in the Spark shell to inspect the result. The resulting table shows the verification result for every test, for example:</p> 
<table border="1" cellpadding="10"> 
 <tbody> 
  <tr style="background-color: #000000"> 
   <td style="text-align: center"><span style="color: #ffffff"><strong>constraint</strong></span></td> 
   <td style="text-align: center"><span style="color: #ffffff"><strong>constraint_status</strong></span></td> 
   <td style="text-align: center"><span style="color: #ffffff"><strong>constraint_message</strong></span></td> 
  </tr> 
  <tr> 
   <td>SizeConstraint(Size(None))</td> 
   <td>Success</td> 
   <td></td> 
  </tr> 
  <tr> 
   <td>MinimumConstraint(Minimum(star_rating,None))</td> 
   <td>Success</td> 
   <td></td> 
  </tr> 
  <tr> 
   <td>MaximumConstraint(Maximum(star_rating,None))</td> 
   <td>Success</td> 
   <td></td> 
  </tr> 
  <tr> 
   <td>CompletenessConstraint(Completeness(review_id,None))</td> 
   <td>Success</td> 
   <td></td> 
  </tr> 
  <tr> 
   <td>UniquenessConstraint(Uniqueness(List(review_id)))</td> 
   <td>Failure</td> 
   <td>Value: 0.9926566948782706 does not meet the constraint requirement!</td> 
  </tr> 
  <tr> 
   <td>CompletenessConstraint(Completeness(marketplace,None))</td> 
   <td>Success</td> 
   <td></td> 
  </tr> 
  <tr> 
   <td>ComplianceConstraint(Compliance(marketplace contained in US,UK,DE,JP,FR,marketplace IS NULL OR marketplace IN (‘US’,’UK’,’DE’,’JP’,’FR’),None))</td> 
   <td>Success</td> 
   <td></td> 
  </tr> 
  <tr> 
   <td>ComplianceConstraint(Compliance(year is non-negative,COALESCE(year, 0.0) &gt;= 0,None))</td> 
   <td>Success</td> 
   <td></td> 
  </tr> 
 </tbody> 
</table> 
<p>Interestingly, the <tt>review_id</tt> column is not unique, which resulted in a failure of the check on uniqueness.</p> 
<p>We can also look at all the metrics that Deequ computed for this check:</p> 
<p><tt>VerificationResult.successMetricsAsDataFrame(spark, verificationResult).show(truncate=False)</tt></p> 
<p>Result:</p> 
<table border="1" cellpadding="10"> 
 <tbody> 
  <tr style="background-color: #000000"> 
   <td style="text-align: center"><span style="color: #ffffff"><strong>name</strong></span></td> 
   <td style="text-align: center"><span style="color: #ffffff"><strong>instance</strong></span></td> 
   <td style="text-align: center"><span style="color: #ffffff"><strong>value</strong></span></td> 
  </tr> 
  <tr> 
   <td>Completeness</td> 
   <td>review_id</td> 
   <td>1</td> 
  </tr> 
  <tr> 
   <td>Completeness</td> 
   <td>marketplace</td> 
   <td>1</td> 
  </tr> 
  <tr> 
   <td>Compliance</td> 
   <td>marketplace contained in US,UK,DE,JP,FR</td> 
   <td>1</td> 
  </tr> 
  <tr> 
   <td>Compliance</td> 
   <td>year is non-negative</td> 
   <td>1</td> 
  </tr> 
  <tr> 
   <td>Maximum</td> 
   <td>star_rating</td> 
   <td>5</td> 
  </tr> 
  <tr> 
   <td>Minimum</td> 
   <td>star_rating</td> 
   <td>1</td> 
  </tr> 
  <tr> 
   <td>Size</td> 
   <td>*</td> 
   <td>3120938</td> 
  </tr> 
  <tr> 
   <td>Uniqueness</td> 
   <td>review_id</td> 
   <td>0.99266</td> 
  </tr> 
 </tbody> 
</table> 
<h3><strong>Automated Constraint Suggestion</strong></h3> 
<p>If you own a large number of datasets or if your dataset has many columns, it may be challenging for you to manually define appropriate constraints. Deequ can automatically suggest useful constraints based on the data distribution. Deequ first runs a data profiling method and then applies a set of rules on the result. For more information about how to run a data profiling method, see <u><a href="https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/examples/data_profiling_example.md" target="_blank" rel="noopener">this GitHub repository.</a></u></p> 
<div class="hide-language"> 
 <pre><code class="lang-scala">import com.amazon.deequ.suggestions.{ConstraintSuggestionRunner, Rules}
import spark.implicits._ // for toDS method

// We ask deequ to compute constraint suggestions for us on the data
val suggestionResult = { ConstraintSuggestionRunner()
  // data to suggest constraints for
  .onData(dataset)
  // default set of rules for constraint suggestion
  .addConstraintRules(Rules.DEFAULT)
  // run data profiling and constraint suggestion
  .run()
}

// We can now investigate the constraints that Deequ suggested. 
val suggestionDataFrame = suggestionResult.constraintSuggestions.flatMap { 
  case (column, suggestions) =&gt; 
    suggestions.map { constraint =&gt;
      (column, constraint.description, constraint.codeForConstraint)
    } 
}.toSeq.toDS()</code></pre> 
</div> 
<p>The result contains a list of constraints with descriptions and Scala code, so that you can directly apply it in your data quality checks. Call <tt>suggestionDataFrame.show(truncate=false)</tt> in the Spark shell to inspect the suggested constraints; here we show a subset:</p> 
<table border="1" cellpadding="10"> 
 <tbody> 
  <tr style="background-color: #000000"> 
   <td style="text-align: center"><span style="color: #ffffff"><strong>column</strong></span></td> 
   <td style="text-align: center" width="244"><span style="color: #ffffff"><strong>constraint</strong></span></td> 
   <td style="text-align: center" width="302"><span style="color: #ffffff"><strong>scala code</strong></span></td> 
  </tr> 
  <tr> 
   <td>customer_id</td> 
   <td width="244">‘customer_id’ is not null</td> 
   <td width="302"><tt>.isComplete(&quot;customer_id&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td>customer_id</td> 
   <td width="244">‘customer_id’ has type Integral</td> 
   <td width="302"><tt>.hasDataType(&quot;customer_id&quot;, ConstrainableDataTypes.Integral)</tt></td> 
  </tr> 
  <tr> 
   <td>customer_id</td> 
   <td width="244">‘customer_id’ has no negative values</td> 
   <td width="302"><tt>.isNonNegative(&quot;customer_id&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td>helpful_votes</td> 
   <td width="244">‘helpful_votes’ is not null</td> 
   <td width="302"><tt>.isComplete(&quot;helpful_votes&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td>helpful_votes</td> 
   <td width="244">‘helpful_votes’ has no negative values</td> 
   <td width="302"><tt>.isNonNegative(&quot;helpful_votes&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td>marketplace</td> 
   <td width="244">‘marketplace’ has value range ‘US’, ‘UK’, ‘DE’, ‘JP’, ‘FR’</td> 
   <td width="302"><tt>.isContainedIn(&quot;marketplace&quot;, Array(&quot;US&quot;, &quot;UK&quot;, &quot;DE&quot;, &quot;JP&quot;, &quot;FR&quot;))</tt></td> 
  </tr> 
  <tr> 
   <td>product_title</td> 
   <td width="244">‘product_title’ is not null</td> 
   <td width="302"><tt>.isComplete(&quot;product_title&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td>star_rating</td> 
   <td width="244">‘star_rating’ is not null</td> 
   <td width="302"><tt>.isComplete(&quot;star_rating&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td>star_rating</td> 
   <td width="244">‘star_rating’ has no negative values</td> 
   <td width="302"><tt>.isNonNegative(&quot;star_rating&quot;)</tt></td> 
  </tr> 
  <tr> 
   <td>vine</td> 
   <td width="244">‘vine’ has value range ‘N’, ‘Y’</td> 
   <td width="302"><tt>.isContainedIn(&quot;vine&quot;, Array(&quot;N&quot;, &quot;Y&quot;))</tt></td> 
  </tr> 
 </tbody> 
</table> 
<p>Note that the constraint suggestion is based on heuristic rules and assumes that the data it is shown is correct, which might not be the case. We recommend to review the suggestions before applying them in production.</p> 
<h2><strong>More Examples on GitHub</strong></h2> 
<p>You can find examples of more advanced features at <u><a href="https://github.com/awslabs/deequ" target="_blank" rel="noopener">Deequ’s GitHub page</a></u>:</p> 
<ul> 
 <li>Deequ not only provides data quality checks with fixed thresholds. Learn how to use <u><a href="https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/examples/anomaly_detection_example.md" target="_blank" rel="noopener">anomaly detection on data quality metrics</a></u> to apply tests on metrics that change over time.</li> 
 <li>Deequ offers support for storing and loading metrics. Learn how to use the <u><a href="https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/examples/metrics_repository_example.md" target="_blank" rel="noopener">MetricsRepository</a></u> for this use case.</li> 
 <li>If your dataset grows over time or is partitioned, you can use Deequ’s <u><a href="https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/examples/algebraic_states_example.md" target="_blank" rel="noopener">incremental metrics computation</a></u> capability. For each partition, Deequ stores a state for each computed metric. To compute metrics for the union of partitions, Deequ can use these states to efficiently derive overall metrics without reloading the data.</li> 
</ul> 
<h3><strong>Additional Resources</strong></h3> 
<p>Learn more about the inner workings of Deequ in our VLDB 2018 paper “<u><a href="http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf" target="_blank" rel="noopener">Automating large-scale data quality verification.</a></u>”</p> 
<h2><strong>Conclusion</strong></h2> 
<p>This blog post showed you how to use Deequ for calculating data quality metrics, verifying data quality metrics, and profiling data to automate the configuration of data quality checks. Deequ is available for you now to build your own data quality management pipeline.</p> 
<p>&nbsp;</p> 
<hr /> 
<h3>About the Authors</h3> 
<p><strong><img class="size-full wp-image-6910 alignleft" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/10/langed.png" alt="" width="113" height="150" />Dustin Lange is an Applied Science Manager at Amazon Search in Berlin. </strong>Dustin’s team develops algorithms for improving the search customer experience through machine learning and data quality tracking. He completed his PhD in similarity search in databases in 2013 and started his Amazon career as an Applied Scientist in forecasting the same year.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p><strong> <img class="size-full wp-image-6913 alignleft" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/10/Sebastian.png" alt="" width="113" height="149" /></strong><strong>Sebastian Schelter is a Senior Applied Scientist at Amazon Search</strong>, working on problems at the intersection of data management and machine learning. He holds a Ph.D. in large-scale data processing from TU Berlin and is an elected member of the Apache Software Foundation, where he currently serves as a mentor for the Apache Incubator.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p><strong><img class="size-full wp-image-6915 alignleft" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/10/phschmid.png" alt="" width="113" height="153" />Philipp Schmidt is an ML Engineer at Amazon Search</strong>. After his graduation from TU Berlin he worked at the University of Potsdam and in several startups in Berlin. At Amazon he is working on enabling data quality tracking for large scale datasets and refining the customer shopping experience through machine learning.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p><strong><img class="size-full wp-image-6912 alignleft" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/05/10/Tammo.png" alt="" width="113" height="159" />Tammo Rukat is an Applied Scientist at Amazon Search in Berlin. </strong>He holds a PhD in statistical machine learning from the University of Oxford. At Amazon he makes use of the abundance and complexity of the company’s large-scale noisy datasets to contribute to a more intelligent customer experience.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p>